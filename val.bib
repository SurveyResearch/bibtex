
@article{bennett_consequences_2012,
	title = {Consequences That Cannot Be Avoided: A Response to Paul Newton},
	volume = {10},
	shorttitle = {Consequences That Cannot Be Avoided},
	doi = {10.1080/15366367.2012.686865},
	number = {1-2},
	journal = {Measurement: Interdisciplinary Research \& Perspective},
	author = {Bennett, Randy Elliot},
	year = {2012},
	keywords = {Validity},
	pages = {30--32}
},

@article{black_epma_2012,
	title = {{EPMA} {Professionals—Servants} or Masters?},
	volume = {10},
	doi = {10.1080/15366367.2012.677342},
	number = {1-2},
	journal = {Measurement: Interdisciplinary Research \& Perspective},
	author = {Black, Paul},
	year = {2012},
	keywords = {Validity},
	pages = {33--37}
},

@article{borsboom_whose_2012,
	title = {Whose Consensus Is It Anyway? Scientific Versus Legalistic Conceptions of Validity},
	volume = {10},
	shorttitle = {Whose Consensus Is It Anyway?},
	doi = {10.1080/15366367.2012.681971},
	number = {1-2},
	journal = {Measurement: Interdisciplinary Research \& Perspective},
	author = {Borsboom, Denny},
	year = {2012},
	keywords = {Validity},
	pages = {38--41}
},

@article{borsboom_theoretical_2003,
	title = {The theoretical status of latent variables},
	volume = {110},
	doi = {10.1037/0033-295X.110.2.203},
	abstract = {This article examines the theoretical status of latent variables as used in modern test theory models. First, it is argued that a consistent interpretation of such models requires a realist ontology for latent variables. Second, the relation between latent variables and their indicators is discussed. It is maintained that this relation can be interpreted as a causal one but that in measurement models for interindividual differences the relation does not apply to the level of the individual person. To substantiate intraindividual causal conclusions, one must explicitly represent individual level processes in the measurement model. Several research strategies that may be useful in this respect are discussed, and a typology of constructs is proposed on the basis of this analysis. The need to link individual processes to latent variable models for interindividual differences is emphasized.},
	number = {2},
	journal = {Psychological Review},
	author = {Borsboom, Denny and Mellenbergh, Gideon J. and van Heerden, Jaap},
	year = {2003},
	keywords = {Causality, individual processes, interindividual differences, Latent variable, Measurement, Measurement Theory, {NORC}, realist ontology, theoretical status, Validity, Variables},
	pages = {203--219}
},

@article{bramley_measurement_2012,
	title = {{“Measurement”} and {“Construct”} Need to Be Clarified First. Commentary on Newton, P. E. {“Clarifying} the Consensus Definition of Validity”},
	volume = {10},
	doi = {10.1080/15366367.2012.677344},
	number = {1-2},
	journal = {Measurement: Interdisciplinary Research \& Perspective},
	author = {Bramley, Tom},
	year = {2012},
	keywords = {Validity},
	pages = {42--45}
},

@article{braun_conceptions_2012,
	title = {Conceptions of Validity: The Private and the Public},
	volume = {10},
	shorttitle = {Conceptions of Validity},
	doi = {10.1080/15366367.2012.679159},
	number = {1-2},
	journal = {Measurement: Interdisciplinary Research \& Perspective},
	author = {Braun, Henry},
	year = {2012},
	keywords = {Validity},
	pages = {46--49}
},

@book{carmines_reliability_1979,
	title = {Reliability and Validity Assessment},
	abstract = {This guide explains how social scientists can evaluate the reliability and validity of empirical measurements, discussing the three basic types of validity: criterion related, content, and construct. In addition, the paper shows how reliability is assessed by the retest method, alternative-forms procedure, split-halves approach, and internal consistency method.},
	publisher = {{SAGE}},
	author = {Carmines, Edward G. and Zeller, Richard A.},
	month = nov,
	year = {1979},
	keywords = {Assessment, Measurement, Reliability, Validity}
},

@article{castillo-diaz_how_????,
	title = {How Cognitive Interviewing can Provide Validity Evidence of the Response Processes to Scale Items},
	doi = {10.1007/s11205-012-0184-8},
	abstract = {The current theory about validity reflected in the Standards for Educational and Psychological Testing ({AERA} et al. in Standards for educational and psychological testing, American Psychological Association, Washington, {DC}, 1999), offers no clear indications about the methods for gathering validity evidence about the response processes. Cognitive interviewing ({CI)} can play an important role answering the current demand about empirical and theoretical analyses of the response processes as a source of validity evidence in psychological testing. {CI} can provide validity evidence for investigating substantive aspects of construct validity and for contributing to the explanations for item and test scores (Zumbo in Handbook of statistics, vol 26, Elsevier, Amsterdam, pp. 45–79, 2007; The concept of validity: revisions, new directions and applications, {IAP—Information} Age Publishing Inc., Charlotte, {NC}, pp. 65–82, 2009). The aim of the study was to illustrate the use of cognitive interviewing method for gathering validity evidence on response processes. The search for evidence about the “response process” was guided by an argument-based approach to validity (Kane in Psychological Bulletin 1992; Educational measurement, American Council on {Education/Praeger}, Washington, {DC}, pp. 17–64, 2006). 21 cognitive interviews were carried out during the cognitive testing of the {APGAR} psychological scale intended to measure the “family support” construct. Cognitive interviewing provided validity evidence that explains how respondents interpret and respond to the {APGAR} items. Respondents maintained a shared interpretation of “family concept” while answering the {APGAR} scale items. Nevertheless, they included in the concept of family not only family members they live with but also other family members and even friends. {CI} participants were also capable of classifying their answers about the family support perception following a polythomous response system. Lastly, the role of {CI} in the Kane’s argument-based approach and Zumbo’s contextualized view of validity will be discussed.},
	journal = {Soc Indic Res},
	author = {Castillo-Díaz, Miguel and Padilla, José-Luis},
	keywords = {{APGAR}, Cognitive Interviewing, Cognitive response processes, Construct validity, Human Geography, Microeconomics, Public Health, Quality of Life Research, Sociology, general, Validity},
	pages = {1--13}
},

@article{cho_validity_2006,
	title = {Validity in qualitative research revisited},
	volume = {6},
	doi = {10.1177/1468794106065006},
	abstract = {Concerns with the issues of validity in qualitative research have dramatically increased. Traditionally, validity in qualitative research involved determining the degree to which researchers’ claims about knowledge corresponded to the reality (or research participants’ construction of reality) being studied.  The authors note that recent trends have shown the emergence of two quite different approaches to the validity question within the literature on qualitative research. The authors categorize and label these ‘transactional’ validity and ‘transformational’ validity. While useful, the authors assert that neither approach is sufficient to meet the current needs of the field. The authors propose a recursive, process-oriented view of validity as an alternative framework.},
	number = {3},
	journal = {Qualitative Research},
	author = {Cho, Jeasik and Trent, Allen},
	year = {2006},
	keywords = {Qualitative Research, Validity},
	pages = {319 --340}
},


@article{cook_current_2006,
	title = {Current Concepts in Validity and Reliability for Psychometric Instruments: Theory and Application},
	volume = {119},
	shorttitle = {Current Concepts in Validity and Reliability for Psychometric Instruments},
	doi = {10.1016/j.amjmed.2005.10.036},
	abstract = {Validity and reliability relate to the interpretation of scores from psychometric instruments (eg, symptom scales, questionnaires, education tests, and observer ratings) used in clinical practice, research, education, and administration. Emerging paradigms replace prior distinctions of face, content, and criterion validity with the unitary concept “construct validity,” the degree to which a score can be interpreted as representing the intended underlying construct. Evidence to support the validity argument is collected from 5 sources: • Content: do instrument items completely represent the construct? • Response process: the relationship between the intended construct and the thought processes of subjects or observers • Internal structure: acceptable reliability and factor structure • Relations to other variables: correlation with scores from another instrument assessing the same construct • Consequences: do scores really make a difference?  Evidence should be sought from a variety of sources to support a given interpretation. Reliable scores are necessary, but not sufficient, for valid interpretation. Increased attention to the systematic collection of validity evidence for scores from psychometric instruments will improve assessments in research, patient care, and education.},
	number = {2},
	journal = {The American Journal of Medicine},
	author = {Cook, David A. and Beckman, Thomas J.},
	month = feb,
	year = {2006},
	keywords = {Construct validity, Educational measurement, Psychometrics, Questionnaire, Reliability, Replication, Reproducibility of Results, Validity},
	pages = {166.e7--166.e16}
},

@article{cramer_why_2012,
	title = {Why the Item “23 +1” Is Not in a Depression Questionnaire: Validity From a Network Perspective},
	volume = {10},
	shorttitle = {Why the Item “23 +1” Is Not in a Depression Questionnaire},
	doi = {10.1080/15366367.2012.681973},
	number = {1-2},
	journal = {Measurement: Interdisciplinary Research \& Perspective},
	author = {Cramer, Angélique O. J.},
	year = {2012},
	keywords = {Validity},
	pages = {50--54}
},

@article{cronbach_construct_1955,
	title = {Construct validity in psychological tests},
	volume = {52},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/13245896},
	number = {4},
	urldate = {2012-03-17},
	journal = {Psychol Bull},
	author = {Cronbach, Lee J. and Meehl, Paul E.},
	month = jul,
	year = {1955},
	note = {{PMID:} 13245896},
	keywords = {Construct validity, Psychometrics, Validity},
	pages = {281--302}
},

@article{engelhard_epistemic_2012,
	title = {Epistemic Iterations and Consensus Definitions of Validity},
	volume = {10},
	doi = {10.1080/15366367.2012.681974},
	number = {1-2},
	journal = {Measurement: Interdisciplinary Research \& Perspective},
	author = {Engelhard, George and Behizadeh, Nadia},
	year = {2012},
	keywords = {Validity},
	pages = {55--58}
},

@article{fischer_standardization_2010,
	title = {Standardization in psychological research.},
	volume = {3},
	url = {http://mvint.usbmed.edu.co:8002/ojs/index.php/web/article/view/463},
	abstract = {Standardization in psychological research.},
	number = {1},
	urldate = {2013-12-09},
	journal = {International Journal of Psychological Research},
	author = {Fischer, Ronald and Milfont, Taciano L.},
	month = jul,
	year = {2010},
	abstract = {The term standardization has been used in a number of different ways in psychological research, mainly in relation to standardization of procedure, standardization of interpretation and standardization of scores. The current paper will discuss the standardization of scores in more detail. Standardization of scores is a common praxis in settings where researchers are concerned with different response styles, issues of faking or social desirability. In these contexts, scores are transformed to increase validity prior to data analysis. In this paper, we will outline a broad taxonomy of standardization methods, will discuss when and how scores can be standardized, and what statistical tests are available after the transformation. Simple step-by-step procedures and examples of syntax files for {SPSS} are provided. Applications for personality, organizational and cross-cultural psychology will be discussed. Limitations of these techniques are discussed, especially in terms of theoretical interpretation of the transformed scores and use of such scores with multivariate statistics.},
	keywords = {response styles, score transformations, social desirability, standardization, Validity, z scores},
	pages = {88--96}
},

@article{freeman_validity_2011,
	title = {Validity in Dialogic Encounters With Hermeneutic Truths},
	volume = {17},
	doi = {10.1177/1077800411409887},
	abstract = {Hermeneutic theories of interpretation are at the core of qualitative methodologies and can be identified as belonging to either epistemological or ontological philosophical orientations. Concerns about validity in qualitative research have mainly been shaped by epistemological questions. What differentiates philosophical hermeneutics, an ontological perspective, from traditional hermeneutics is its radical departure from finding a “technique” of interpretation to proposing a hermeneutic ontology, where the hermeneutic task of understanding is thought to be our very way of being in the world. Unlike traditional interpretive approaches which often seek to maximize validity by eliciting a respondent’s account of an experience in a way that closely corresponds to that experience, philosophical hermeneutical approaches assert that the meaning of the experience is uniquely configured in the dialogic encounter itself. Dialogue is thought to offer a hermeneutic valence for people’s engagement with understanding and the means of encountering truth. Understanding, therefore, cannot be conceived of as a fixing of meaning but as an event in which meaning is generated and transformed. This article considers how philosophical hermeneutics might inform qualitative research when the aim is to understand.},
	number = {6},
	journal = {Qualitative Inquiry},
	author = {Freeman, Melissa},
	month = jul,
	year = {2011},
	keywords = {Dialogics, Hermeneutics, Qualitative Analysis, Validity},
	pages = {543 --551}
},

@article{gorin_reconsidering_2007,
	title = {Reconsidering Issues in Validity Theory},
	volume = {36},
	doi = {10.3102/0013189X07311607},
	abstract = {Lissitz and Samuelsen (2007) propose a new framework for validity theory and terminology, emphasizing a shift in the and practice toward issues of test content rather than constructs. The author of this article argues that several of Lissitz and Samuelsen's critiques of validity theory focus on previously considered, but subsequently discarded, validity conceptualizations. In addition, she suggests that Lissitz and Samuelsen's conceptualization returns to methods shown historically to be problematic for score use and interpretation. In doing so, she highlights developments in validity theory and practice centering on cognitively based examinations of test scores that have contributed to increased understanding of score meaning and stronger validity arguments.},
	number = {8},
	journal = {Educational Researcher},
	author = {Gorin, Joanna S.},
	month = nov,
	year = {2007},
	keywords = {Psychometrics, Validity},
	pages = {456--462}
},

@article{haig_construct_2012,
	title = {From Construct Validity to Theory Validation},
	volume = {10},
	doi = {10.1080/15366367.2012.681975},
	number = {1-2},
	journal = {Measurement: Interdisciplinary Research \& Perspective},
	author = {Haig, Brian D.},
	year = {2012},
	keywords = {Construct validity, Validity},
	pages = {59--62}
},

@article{herrman_fundamentals_2009,
	title = {Fundamentals of Methodology Part I: Definitions and First Principles},
	shorttitle = {Fundamentals of Methodology - Part I},
	url = {http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1373976},
	abstract = {This eight-part series of papers presents a selection of semantic, analytic and metaphysical considerations at the heart of methodology. By 'selection' I mean that they are what a structuralist considers to be its salient and necessary tools and principles - not necessarily those that the broader 'community of scholars' finds companionable. The first paper offers a review of the series followed by analytical definitions featuring - relation, fact, principle, paradigm, methodology and theory. It concludes with a look at some 'first principles' governing paradigmatics. Future numbers treat generally of structures and signs-the metaphysical considerations necessary to paradigmatic analysis. Papers devoted to specific methods include one on frames, another on proverbs and parables. Another treats of interpolation, extrapolation and form-content analysis. Notably missing from this treatment are specialized tools of higher mathematics and formal logic that are the darlings of modern philosophy. They have their place and utility - just not in philosophy.},
	urldate = {2012-01-21},
	journal = {{SSRN} {eLibrary}},
	author = {Herrman, C. S.},
	month = apr,
	year = {2009},
	keywords = {Fact, Methodology, Paradigms, Relation, scientific method, theory, Validity}
},

@article{kane_all_2012,
	title = {All Validity Is Construct Validity. Or Is It?},
	volume = {10},
	issn = {1536-6367},
	doi = {10.1080/15366367.2012.681977},
	number = {1-2},
	journal = {Measurement: Interdisciplinary Research \& Perspective},
	author = {Kane, Michael},
	year = {2012},
	keywords = {Construct validity, Validity},
	pages = {66--70}
},

@article{king_enhancing_2004,
	title = {Enhancing the Validity and Cross-cultural Comparability of Measurement in Survey Research},
	volume = {98},
        doi = {10.1007/978-3-531-91826-6_16},
	abstract = {We address two long-standing survey research problems: measuring complicated concepts, such as political freedom or efficacy, that researchers define best with reference to examples and and what to do when respondents interpret identical questions in different ways. Scholars have long addressed these problems with approaches to reduce incomparability, such as writing more concrete questions – with uneven success. Our alternative is to measure directly response category incomparability and to correct for it. We measure incomparability via respondents’ assessments, on the same scale as the self-assessments to be corrected, of hypothetical individuals described in short vignettes. Since actual levels of the vignettes are invariant over respondents, variability in vignette answers reveals incomparability. Our corrections require either simple recodes or a statistical model designed to save survey administration costs. With analysis, simulations, and cross-national surveys, we show how response incomparability can drastically mislead survey researchers and how our approach can fix them.},
	journal = {American Political Science Review},
	author = {King, Gary and Murray, Christopher J. L. and Salomon, Joshua A. and Tandon, Ajay},
	year = {2004},
	keywords = {Cross-cultural research, Measurement, Survey Research, Validity},
	pages = {191–207}
},

@article{kroon_theory-dependence_2011,
	title = {Theory-dependence, warranted reference, and the epistemic dimensions of realism},
	volume = {1},
	doi = {10.1007/s13194-010-0004-4},
	abstract = {The question of the role of theory in the determination of reference of theoretical terms continues to be a controversial one. In the present paper I assess a number of responses to this question (including variations on David Lewis’s appeal to Ramsification), before describing an alternative, epistemically oriented account of the reference-determination of such terms. The paper concludes by discussing some implications of the account for our understanding of both realism and such competitors of realism as constructive empiricism.},
	number = {2},
	journal = {Euro Jnl Phil Sci},
	author = {Kroon, Frederick},
	month = may,
	year = {2011},
	keywords = {Constructive empiricism, Fictionalism, Meaning, Philosophy of Science, Realism, reference, Theoretical terms, Validity},
	pages = {173--191}
},

@article{lane_consequences_2012,
	title = {Consequences of Assessment and Accountability Systems Are Integral to the Argument-Based Approach to Validity},
	volume = {10},
	doi = {10.1080/15366367.2012.679160},
	number = {1-2},
	urldate = {2013-01-24},
	journal = {Measurement: Interdisciplinary Research \& Perspective},
	author = {Lane, Suzanne},
	year = {2012},
	keywords = {Validity},
	pages = {71--74}
},


@article{maraun_validity_2012,
	title = {Validity and Measurement},
	volume = {10},
	issn = {1536-6367},
	doi = {10.1080/15366367.2012.682523},
	number = {1-2},
	journal = {Measurement: Interdisciplinary Research \& Perspective},
	author = {Maraun, Michael D.},
	year = {2012},
	keywords = {Measurement, Validity},
	pages = {80--83}
},

@incollection{margulis_measurement_2010,
	address = {Oxford},
	title = {Measurement Error and Reliability},
	url = {http://www.sciencedirect.com/science/article/pii/B978008045337800231X},
	abstract = {The common perception of behavioral data collection as a subjective process, data being largely descriptive and not amenable to quantitative analysis, is false. The clearly defined methods of observational sampling provide the necessary framework within which to collect quantitative behavioral data. However, as with any scientific endeavor, such methods are subject to various sources of error. It is important to recognize these sources of error in order to control them during the data collection process. Techniques exist to address these various sources of error and bias, and enable behavioral researchers to compile data that are reliable, repeatable, and unbiased.},
	booktitle = {Encyclopedia of Animal Behavior},
	publisher = {Academic Press},
	author = {Margulis, {S.W.}},
	editor = {Editors-in-Chief:  Michael D. Breed and Janice Moore},
	year = {2010},
	keywords = {Error theory, Errors of apprehension, Ethogram, Ethology, Interobserver reliability, Intraobserver reliability, Measurement, Measures of concordance, Measures of correlation, Observational sampling methods, Observer bias, Observer error, Validity},
	pages = {424--428}
},

@article{murphy_validity_2012,
	title = {Validity for What? The Peril of Overclarifying},
	volume = {10},
	shorttitle = {Validity for What?},
	doi = {10.1080/15366367.2012.677362},
	number = {1-2},
	journal = {Measurement: Interdisciplinary Research \& Perspective},
	author = {Murphy, Kevin R.},
	year = {2012},
	keywords = {Validity},
	pages = {97--99}
},

@article{parry_validity_1950,
	title = {Validity of Responses to Survey Questions},
	volume = {14},
	doi = {10.1086/266150},
	abstract = {This article is designed as one of a series which will discuss certain aspects of validity in surveys. The first article, which appears below, examines two current concepts of validity (as predictive accuracy, and as a matter of interpretation), reviews the literature on the subject, and presents some of the results of a specially-designed survey in Denver which showed that the validity of even simple "factual" responses may often be open to question. Subsequent articles will discuss the effect of the interviewer on the validity of survey results and the variations in validity according to respondent characteristics and other variables.},
	number = {1},
	journal = {The Public Opinion Quarterly},
	author = {Parry, Hugh J. and Crossley, Helen M.},
	month = apr,
	year = {1950},
	keywords = {{NORC}, Questionnaire Validation, Reliability, Response Validity, Survey Validity, Validity},
	pages = {61--80}
},

@book{pedhazur_measurement_1991,
	title = {Measurement, design, and analysis: an integrated approach},
	isbn = {9780805810639},
	shorttitle = {Measurement, design, and analysis},
	abstract = {In textbooks and courses in statistics, substantive and measurement issues are rarely, if at all, considered. Similarly, textbooks and courses in measurement virtually ignore design and analytic questions, and research design textbooks and courses pay little attention to analytic and measurement issues. This fragmentary approach fosters a lack of appreciation of the interrelations and interdependencies among the various aspects of the research endeavor. Pedhazur and Schmelkin's goal is to help readers become proficient in these aspects of research and their interrelationships, and to use that information in a more integrated manner. The authors offer extensive commentaries on inputs and outputs of computer programs in the context of the topics presented. Both the organization of the book and the style of presentation allow for much flexibility in choice, sequence, and degree of sophistication with which topics are dealt.},
	publisher = {Psychology Press},
	author = {Pedhazur, Elazar J. and Schmelkin, Liora Pedhazur},
	year = {1991},
	keywords = {Measurement, {NORC}, Survey Methodology, Validity}
},

@article{pollitt_validity_2012,
	title = {Validity Cannot Be Created, It Can Only Be Lost},
	volume = {10},
	doi = {10.1080/15366367.2012.686868},
	number = {1-2},
	urldate = {2013-01-24},
	journal = {Measurement: Interdisciplinary Research \& Perspective},
	author = {Pollitt, Alastair},
	year = {2012},
	keywords = {Validity},
	pages = {100--103}
},

@article{romney_culture_1986,
	series = {New Series},
	title = {Culture as Consensus: A Theory of Culture and Informant Accuracy},
	volume = {88},
	doi = {10.1525/aa.1986.88.2.02a00020},
	shorttitle = {Culture as Consensus},
	abstract = {This paper presents and tests a formal mathematical model for the analysis of informant responses to systematic interview questions. We assume a situation in which the ethnographer does not know how much each informant knows about the cultural domain under consideration nor the answers to the questions. The model simultaneously provides an estimate of the cultural competence or knowledge of each informant and an estimate of the correct answer to each question asked of the informant. The model currently handles true-false, multiple-choice, and fill-in-the-blank type question formats. In familiar cultural domains the model produces good results from as few as four informants. The paper includes a table showing the number of informants needed to provide stated levels of confidence given the mean level of knowledge among the informants. Implications are discussed.},
	number = {2},
	journal = {American Anthropologist},
	author = {Romney, A. Kimball and Weller, Susan C. and Batchelder, William H.},
	month = jun,
	year = {1986},
	keywords = {cultural consensus model, Reliability, reliability of informants, Validity},
	pages = {313--338}
},

@article{scherpenzeel_validity_1997,
	title = {The Validity and Reliability of Survey Questions A Meta-Analysis of {MTMM} Studies},
	volume = {25},
	doi = {10.1177/0049124197025003004},
	abstract = {Inspired by the research of Frank Andrews on the reliability and validity of survey questions, a large-scale research project was conducted in the Netherlands. The project was comprised of two different stages. For this project, more than 600 survey questions were included in different surveys according to a multitrait-multimethod design. The resulting data were analyzed in two steps. In the first step, estimates of validity and reliability were obtained for each question. The second step was a meta-analysis of the variation in data quality found in the first step. This variation was related to question-specific characteristics, response scale characteristics, context characteristics, and design characteristics. The article describes how the results of this study can be of practical use. In addition, the authors compare them to results of similar studies in the United States, Austria, and other Western, Central, and Eastern European countries.},
	number = {3},
	journal = {Sociological Methods \& Research},
	author = {Scherpenzeel, Annette C and Saris, Willem E},
	month = feb,
	year = {1997},
	keywords = {{NORC}, Questions, Reliability, Survey Methodology, Validity},
	pages = {341--383}
},

@article{sijtsma_correcting_2009,
	title = {Correcting Fallacies in Validity, Reliability, and Classification},
	volume = {9},
	doi = {10.1080/15305050903106883},
	abstract = {This article reviews three topics from test theory that continue to raise discussion and controversy and capture test theorists' and constructors' interest. The first topic concerns the discussion of the methodology of investigating and establishing construct validity; the second topic concerns reliability and its misuse, alternative definitions of reliability, and methods for estimating reliability; and the third topic concerns the relationships between reliability, test length, and the insufficient quality of decision making using short but reliable tests.
This article reviews three topics from test theory that continue to raise discussion and controversy and capture test theorists' and constructors' interest. The first topic concerns the discussion of the methodology of investigating and establishing construct validity; the second topic concerns reliability and its misuse, alternative definitions of reliability, and methods for estimating reliability; and the third topic concerns the relationships between reliability, test length, and the insufficient quality of decision making using short but reliable tests.},
	number = {3},
	journal = {International Journal of Testing},
	author = {Sijtsma, Klaas},
	year = {2009},
	keywords = {classification, Reliability, Validity},
	pages = {167--194}
},

@article{strauss_construct_2009,
	title = {Construct Validity: Advances in Theory and Methodology},
	volume = {5},
	shorttitle = {Construct Validity},
	doi = {10.1146/annurev.clinpsy.032408.153639},
	abstract = {Measures of psychological constructs are validated by testing whether they relate to measures of other constructs as specified by theory. Each test of relations between measures reflects on the validity of both the measures and the theory driving the test. Construct validation concerns the simultaneous process of measure and theory validation. In this article, we review the recent history of validation efforts in clinical psychological science that has led to this perspective, and we review the following recent advances in validation theory and methodology of importance for clinical researchers. These are: the emergence of nonjustificationist philosophy of science; an increasing appreciation for theory and the need for informative tests of construct validity; valid construct representation in experimental psychopathology; the need to avoid representing multidimensional constructs with a single score; and the emergence of effective new statistical tools for the evaluation of convergent and discriminant validity.},
	number = {1},
	journal = {Annual Review of Clinical Psychology},
	author = {Strauss, Milton E. and Smith, Gregory T.},
	year = {2009},
	note = {{PMID:} 19086835},
	keywords = {construct homogeneity, construct representation, construct validation programs, Construct validity, multitrait-multimethod validation, Philosophy of Science, Validity},
	pages = {1--25}
},

@incollection{sundholm_inference_1998,
	address = {Prague},
	title = {Inference versus Consequence},
	url = {https://openaccess.leidenuniv.nl/handle/1887/10419},
	urldate = {2012-01-16},
	booktitle = {{LOGICA} Yearbook 1997, 26 - 35 (1998)},
	publisher = {Filosofia Publishers, Czech Academy of Science},
	author = {Sundholm, B. G},
	year = {1998},
	keywords = {Inference, Logical Consequence, Validity},
	pages = {26--35},
	file = {Snapshot:/Users/gar/work/bibliography/zotero/storage/P3Q5HPEN/10419.html:text/html;Sundholm-InferenceVersusConsequence.pdf:/Users/gar/Documents/proof theory/Sundholm-InferenceVersusConsequence.pdf:application/pdf}
},

@article{whitely_construct_1983,
	title = {Construct validity: Construct representation versus nomothetic span},
	volume = {93},
	shorttitle = {Construct validity},
	doi = {10.1037/0033-2909.93.1.179},
	abstract = {Presents a new approach to construct validation research: construct modeling. A paradigm shift from functionalism to structuralism in psychology permits 2 types of research to be separated. Construct representation is concerned with identifying the theoretical mechanisms that underlie responses, such as information processes, strategies, and knowledge stores. Three approaches to assessing construct representation are presented: (1) mathematical modeling, particularly as used in cognitive psychology; (2) psychometric modeling, as exemplified by latent trait modeling; and (3) multicomponent latent trait modeling. Nomothetic span is concerned with the network of relationships of a test score with other variables. These 2 types of construct validation research address different issues and require different types of data. For each type of construct validation research, appropriate methods and quantitative models are presented to test a priori hypotheses about construct validity. Examples are presented, and the construct modeling approach is compared with both the traditional psychometric approach and the information-processing approach to establishing theoretical mechanisms in performance. (41 ref) ({PsycINFO} Database Record (c) 2010 {APA}, all rights reserved)},
	number = {1},
	journal = {Psychological Bulletin},
	author = {Whitely, Susan E.},
	year = {1983},
	keywords = {Construct validity, Validity},
	pages = {179--197}
},

@article{zand_scholten_consensus_2012,
	title = {The Consensus Definition Redefined From a Representational Perspective},
	volume = {10},
	doi = {10.1080/15366367.2012.681978},
	number = {1-2},
	journal = {Measurement: Interdisciplinary Research \& Perspective},
	author = {Zand Scholten, Annemarie},
	year = {2012},
	keywords = {Representational theory of measurement, Validity},
	pages = {104--109}
},

@article{zimmerman_commentary_1998,
	title = {Commentary on {‘Science}, Measurement, and Validity: Is Completion of Samuel Messick's Synthesis Possible?’ by Keith A. Markus},
	volume = {45},
	shorttitle = {Commentary on {‘Science}, Measurement, and Validity},
	doi = {10.1023/A:1006977210073},
	number = {1-3},
	journal = {Social Indicators Research},
	author = {Zimmerman, Donald W.},
	month = nov,
	year = {1998},
	keywords = {Microeconomics, Quality of Life Research, Sociology, Validity},
	pages = {69--72}
},

@article{zimmerman_how_1998,
	title = {How Should Classical Test Theory Have Defined Validity?},
	volume = {45},
	doi = {10.1023/A:1006949915525},
	abstract = {Classical test theory defined the predictive validity of a test as the ordinary Pearson correlation between scores on the test and scores on a validation criterion. For some purposes this definition is satisfactory, but for others it leads to complications, because derivation of familiar equations relating validity and reliability requires an independent assumption of uncorrelated errors of measurement. The present paper proposes an alternate definition of validity that avoids difficulties arising from correlated error scores and is more consistent with standard definitions of true score, error score, and reliability in the classical theory.},
	number = {1-3},
	journal = {Social Indicators Research},
	author = {Zimmerman, Donald W.},
	month = nov,
	year = {1998},
	keywords = {Microeconomics, Sociology, Test theory, Validity},
	pages = {233--251}
}