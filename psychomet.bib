@article{blinkhorn_past_1997,
	title = {Past imperfect, future conditional: Fifty years of test theory},
	volume = {50},
	shorttitle = {Past imperfect, future conditional},
	doi = {10.1111/j.2044-8317.1997.tb01139.x},
	abstract = {This essay takes a broad view of developments in test theory over the past 50 years from the point of view of a test constructor. Increasing theoretical and technical sophistication has not, in general, resulted in commensurate improvements in test design. Contemporary test theory, with its emphasis on statistical rather than psychological models, has become inaccessible to the majority of test users, and predominantly reflects educational rather than psychological concerns. Real progress may depend on the emergence of a new and radical reconceptualization.},
	number = {2},
	journal = {British Journal of Mathematical and Statistical Psychology},
	author = {Blinkhorn, S. F.},
	year = {1997},
	keywords = {Psychometrics, Test theory},
	pages = {175–185}
},

@article{borsboom_attack_2006,
	title = {The attack of the psychometricians},
	volume = {71},
	doi = {10.1007/s11336-006-1447-6},
	abstract = {This paper analyzes the theoretical, pragmatic, and substantive factors that have hampered the integration between psychology and psychometrics. Theoretical factors include the operationalist mode of thinking which is common throughout psychology, the dominance of classical test theory, and the use of “construct validity” as a catch-all category for a range of challenging psychometric problems. Pragmatic factors include the lack of interest in mathematically precise thinking in psychology, inadequate representation of psychometric modeling in major statistics programs, and insufficient mathematical training in the psychological curriculum. Substantive factors relate to the absence of psychological theories that are sufficiently strong to motivate the structure of psychometric models. Following the identification of these problems, a number of promising recent developments are discussed, and suggestions are made to further the integration of psychology and psychometrics.},
	number = {3},
	journal = {Psychometrika},
	author = {Borsboom, Denny},
	month = sep,
	year = {2006},
	keywords = {Assessment, Testing and Evaluation, Classical test theory, Construct validity, modern test theory, Psychological measurement, Psychometrics, Statistical Theory and Methods},
	pages = {425--440}
},

@article{borsboom_why_2004,
	title = {Why Psychometrics is Not Pathological A Comment on Michell},
	volume = {14},
	doi = {10.1177/0959354304040200},
	abstract = {This paper comments on an article by Michell (2000), who argues that psychometrics should be qualified as pathological science for two reasons: (a) psychometrics assumes psychological attributes to be quantitative without testing this hypothesis; and (b) the fact that this hypothesis is not tested is disguised. Michell further argues that the hypothesis should be tested using additive conjoint measurement theory. Although relevant to classical test theory, Michell’s arguments do not apply to psychometrics in general. In particular, they are largely irrelevant to item response theory models. We show that these models result from introducing probabilistic relations, which are needed to deal with measurement error, and not from a breakdown in critical inquiry, as Michell suggests. Moreover, at least one class of these models can be formulated in terms of additive conjoint measurement theory, which renders Michell’s call for the additive conjoint model in need of qualification. Finally, item response theory models are routinely tested against empirical data, and although the assumption that an attribute is quantitative cannot be tested directly, such tests do address the conjunction of this assumption and other model assumptions. We conclude that, although Michell’s arguments are important to psychological measurement, they are largely irrelevant to item response theory. In fact, we argue that they can be phrased in terms of this theory in a natural way.},
	number = {1},
	journal = {Theory \& Psychology},
	author = {Borsboom, Denny and Mellenbergh, Gideon J.},
	month = feb,
	year = {2004},
	keywords = {empiricism, item response theory, Measurement, Psychometrics, Realism, representationalism},
	pages = {105--120}
},

@article{chang_translation_1999,
	title = {Translation of questionnaires and issues of equivalence},
	volume = {29},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/10197930},
	abstract = {The validity of studies using translated instruments may be questioned when there is a lack of attention to and/or minimal explanation of the procedures used for determining the equivalence between the primary and secondary language tool. Ensuring equivalence of a translated Chinese version of the Menstrual Distress Questionnaire is an important prerequisite for identifying culturally specific expressions of concepts under investigation and for cross-cultural comparisons. This paper examines the principles and procedures for determining equivalence of translated tools and their application to the development of an equivalent Chinese version of the Menstrual Distress Questionnaire. Translation and back-translation were used to develop a Chinese version of the Menstrual Distress Questionnaire. Bilingual university students completed both versions of the Menstrual Distress Questionnaire. Most of the Menstrual Distress Questionnaire items had an acceptable Kappa of {\textgreater}0.4. Intraclass correlation coefficients indicated moderate to high levels of equivalence for total scores and all scales. Improvement in the translation of some items is needed to further enhance the equivalence of the Chinese version of the Menstrual Distress Questionnaire.},
	number = {2},
	urldate = {2011-08-28},
	journal = {J Adv Nurs},
	author = {Chang, A M and Chau, J P and Holroyd, E},
	month = feb,
	year = {1999},
	note = {{PMID:} 10197930},
	keywords = {Adult, Cross-Cultural Comparison, Cross-Over Studies, Cross-Sectional Studies, Equivalence, Female, Hong Kong, Humans, Premenstrual Syndrome, Psychometrics, Questionnaires, Reproducibility of Results, Retrospective Studies, Statistics, Nonparametric, Translation},
	pages = {316--322}
},

@article{coon_standardizing_1993,
	title = {Standardizing the Subject: Experimental Psychologists, Introspection, and the Quest for a Technoscientific Ideal},
	volume = {34},
	shorttitle = {Standardizing the Subject},
	doi = {10.2307/3106414},
	number = {4},
	journal = {Technology and Culture},
	author = {Coon, Deborah J.},
	month = oct,
	year = {1993},
	keywords = {Psychometrics, Standardized Interview},
	pages = {757}
},

@article{cronbach_construct_1955,
	title = {Construct validity in psychological tests},
	volume = {52},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/13245896},
	number = {4},
	urldate = {2012-03-17},
	journal = {Psychol Bull},
	author = {Cronbach, Lee J. and Meehl, Paul E.},
	month = jul,
	year = {1955},
	note = {{PMID:} 13245896},
	keywords = {Construct validity, Psychometrics, Validity},
	pages = {281--302}
},

@article{dawes_psychological_1994,
	title = {Psychological measurement},
	volume = {101},
	doi = {10.1037/0033-295X.101.2.278},
	abstract = {L. L. Thurstone's (see {PA}, Vol 2:527; see also {PA}, Vol 81:28135) article developed a representational measurement model of comparative judgment; estimated discrimination probabilities yield scale values that imply values of other probabilities not yet observed, if the model provides a true representation. In practice, the accuracy of such inferences is captured by "goodness-of-fit" statistics. The specific representational measurement model developed can yield magnitude measurement on psychological dimensions for which no corresponding physical dimensions exist (e.g., favorability of "attitude toward"). This revolutionary article led to the development of many other representational measurement models. As opposed to psychophysics, however, the introduction of "true measurement" in social, attitudinal, and personality psychology did not yield the rapid progress Thurstone envisioned, and currently this specific model is seldom used in these areas.},
	number = {2},
	journal = {Psychological Review},
	author = {Dawes, Robyn M.},
	year = {1994},
	keywords = {{*Psychophysical} Measurement, Measurement, Psychometrics, Psychophysics},
	pages = {278--281}
},

@book{devellis_scale_2003,
	address = {Thousand Oaks, Calif.},
	edition = {2nd},
	title = {Scale development : theory and applications},
	isbn = {0761926046  9780761926047  0761926054  9780761926054},
	shorttitle = {Scale development},
	abstract = {{'Scale} Development' guides the reader toward the identification of the latent variable, the generation of an item pool, the format for measurement \& the optimization of the scale length. Using exercises to illustrate the concepts, the text also includes advice about factor analytic strategies.},
	publisher = {Sage Publications},
	author = {{DeVellis}, Robert F},
	year = {2003},
	keywords = {Measurement, {NORC}, Psychometrics, Questionnaire Design, Questionnaire Validation, Scale development}
},

@article{dresner_language_2010,
	title = {Language and the Measure of Mind},
	volume = {25},
	doi = {10.1111/j.1468-0017.2010.01396.x},
	abstract = {In his recent book The Measure of Mind Robert Matthews presents the most elaborate and convincing attempt to date to account for the propositional attitudes in measurement theoretic terms. In the first section of this paper I review earlier applications of measurement-theoretic conceptualization to the discussion of the mind, I outline Matthews' own account, and I raise two questions concerning it. Then, in the second section of the paper, I present a unified measurement-theoretic account of both linguistic meaning and the propositional attitudes, in which a variant of Matthews' position is embedded. Such a unified account, I argue, yields satisfactory answers to the questions raised with respect to Matthews' original view, and demonstrates other advantages.},
	number = {4},
	journal = {Mind \& Language},
	author = {Dresner, Eli},
	month = sep,
	year = {2010},
	keywords = {Measurement, Pragmatics, Propositional Attitudes, Propositions, Psychometrics, Survey Methodology},
	pages = {418--439}
},

@article{essex_between_1999,
	title = {Between Numbers and Notions A Critique of Psychological Measurement},
	volume = {9},
	doi = {10.1177/0959354399096002},
	abstract = {When psychologists apply mathematical machinery to psychological ideas, that machinery imposes certain requirements in the linkage of numbers and notions. These impose choices driven by the mathematics and not the psychology. These decisions, forced by the mathematics, induce theoretical issues in the psychology. Attempting a theory-neutral approach to research in psychology, where commitments in response to the options are made unknowingly, thus becomes instead a theory-by-default psychology. This paper begins to catalogue some of these mathematical choices to make them explicit, in order to allow psychologists the opportunity to make explicit theoretical commitments.},
	number = {6},
	journal = {Theory \& Psychology},
	author = {Essex, Christopher and Smythe, William E.},
	month = dec,
	year = {1999},
	keywords = {Measurement, {METHODOLOGY}, metric, operationism, positivism, Psychometrics, Quantification, rank order, theory},
	pages = {739--767}
},

@article{flaherty_developing_1988,
	title = {Developing instruments for cross-cultural psychiatric research},
	volume = {176},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/3367140},
	abstract = {The growth of cross-cultural psychiatry is now occurring at a time when psychiatry in general is emphasizing diagnostic clarity and the use of quantifiable and reliable methods of collecting clinical and research data. It is now imperative that cross-cultural psychiatry also examine its methods for developing instruments for use in cross-cultural research. This paper outlines a method for developing instruments designed in one culture for use in a second, and particular attention is given to cross-cultural validity or equivalence. Five types of equivalence are enumerated and defined: content, semantic, technical, criterion, and conceptual equivalence. These concepts are illustrated by examples from the authors' experience in research on internal migrants in Peru.},
	number = {5},
	urldate = {2011-10-23},
	journal = {J. Nerv. Ment. Dis.},
	author = {Flaherty, J A and Gaviria, F M and Pathak, D and Mitchell, T and Wintrob, R and Richman, J A and Birz, S},
	month = may,
	year = {1988},
	note = {{PMID:} 3367140},
	keywords = {Cross-Cultural Comparison, Cross-cultural research, Humans, Language, Mental Disorders, Peru, Psychiatric Status Rating Scales, Psychological Tests, Psychometrics, Research Design, Social Adjustment, Transients and Migrants, Translation},
	pages = {257--263}
},

@article{gorin_reconsidering_2007,
	title = {Reconsidering Issues in Validity Theory},
	volume = {36},
	url = {http://www.jstor.org/stable/4621100},
	doi = {10.2307/4621100},
	abstract = {Lissitz and Samuelsen (2007) propose a new framework for validity theory and terminology, emphasizing a shift in the and practice toward issues of test content rather than constructs. The author of this article argues that several of Lissitz and Samuelsen's critiques of validity theory focus on previously considered, but subsequently discarded, validity conceptualizations. In addition, she suggests that Lissitz and Samuelsen's conceptualization returns to methods shown historically to be problematic for score use and interpretation. In doing so, she highlights developments in validity theory and practice centering on cognitively based examinations of test scores that have contributed to increased understanding of score meaning and stronger validity arguments.},
	number = {8},
	urldate = {2013-01-27},
	journal = {Educational Researcher},
	author = {Gorin, Joanna S.},
	month = nov,
	year = {2007},
	keywords = {Psychometrics, Validity},
	pages = {456--462}
},

@article{hale_uncertainty_2006,
	title = {Uncertainty about the Rest of the Sentence},
	volume = {30},
	url = {http://www.eric.ed.gov/ERICWebPortal/detail?accno=EJ747089},
	abstract = {A word-by-word human sentence processing complexity metric is presented. This metric formalizes the intuition that comprehenders have more trouble on words contributing larger amounts of information about the syntactic structure of the sentence as a whole. The formalization is in terms of the conditional entropy of grammatical continuations, given the words that have been heard so far. To calculate the predictions of this metric, Wilson and Carroll's (1954) original entropy reduction idea is extended to infinite languages. This is demonstrated with a mildly context-sensitive language that includes relative clauses formed on a variety of grammatical relations across the Accessibility Hierarchy of Keenan and Comrie (1977). Predictions are derived that correlate significantly with repetition accuracy results obtained in a sentence-memory experiment (Keenan \& Hawkins, 1987).},
	number = {4},
	urldate = {2012-03-19},
	journal = {Cognitive Science},
	author = {Hale, John},
	year = {2006},
	keywords = {Associative Learning, Comprehension, Grammar, Language Research, Prediction, Psychometrics, Sentence Structure, Sentences, Word Order},
	pages = {643--672}
},

@article{hoshmand_can_2003,
	title = {Can Lessons of History and Logical Analysis Ensure Progress in Psychological Science?},
	volume = {13},
	doi = {10.1177/0959354303131003},
	abstract = {A cultural view of science adds an important explanatory dimension to the historical account provided by Michell (2003) for the quantitative imperative. It provides an understanding of the rhetoric of paradigm wars as a cultural, sociological phenomenon. The pragmatist view of science further points to the need for evaluating our research praxis so as to determine the relative merits of diverse inquiry approaches in the interest of making improvements. Whether such progress in psychological science would be facilitated by an evolving metamethodological understanding and an awareness of the culture of academic psychology depends on the extent to which the profession will adopt a self-reflective and critical stance.},
	number = {1},
	journal = {Theory \& Psychology},
	author = {Hoshmand, Lisa Tsoi},
	month = feb,
	year = {2003},
	keywords = {Pragmatism, progress, Psychometrics, qualitative research, quantitative imperative},
	pages = {39--44}
},

@article{kline_commentary_1997,
	title = {Commentary on Michell, Quantitative Science and the definition of measurement in psychology},
	volume = {88},
	doi = {10.1111/j.2044-8295.1997.tb02642.x},
	number = {3},
	journal = {British Journal of Psychology},
	author = {Kline, Paul},
	year = {1997},
	keywords = {Measurement, Psychometrics},
	pages = {358–387}
},

@article{laming_critique_1997,
	title = {A critique of a measurement–theoretic critique: Commentary on Michell, Quantitative science and the definition of measurement in psychology},
	volume = {88},
	shorttitle = {A critique of a measurement–theoretic critique},
	doi = {10.1111/j.2044-8295.1997.tb02643.x},
	abstract = {Over the past 40 years there has been much theoretical progress in the understanding of what it means to make measurements. If numbers are assigned to objects or events, the kinds of arithmetical operations (such as averaging or calculating ratios) which it is thereafter meaningful to carry out on the numbers depend on the rule of assignment. Measurement theory, roughly speaking, is concerned to identify what conditions need to be satisfied to make this or that arithmetical operation meaningful. Measurement theorists, generally, feel that psychologists have disregarded their work, to the detriment of the development of psychology as a natural science. Michell's article is a polemic—a very scholarly and well-argued polemic—addressing this issue. It would have helped his argument, however, to have explained why measurement theory should matter to psychologists, and I endeavour, first of all, to remedy that deficit.},
	number = {3},
	journal = {British Journal of Psychology},
	author = {Laming, Donald},
	year = {1997},
	keywords = {Measurement, Psychometrics},
	pages = {389–391}
},

@article{lawrence_three_2010,
	title = {Three approaches to the disjunction between psychological measurement and psychological persons: methodological and ethical considerations},
	volume = {44},
	shorttitle = {Three approaches to the disjunction between psychological measurement and psychological persons},
	doi = {10.1007/s12124-010-9129-4},
	abstract = {In this paper, we take forward Schwarz's (2009) disjunction between measurement-apparatus-questionnaire and measurement-apparatus-man to examine how the crisis in contemporary psychology is related to assumptions about two sets of connections in research: connections between research tools, research behaviours, and psychological phenomena; and connections between researchers and researchees. By setting up a research problem with methodological and ethical implications, we describe three approaches that involve different assumptions and research activities in relation to the ways each makes these connections: Disassociated, Conventionally Connected and Persons in Dialogue Approaches. We argue that a Persons in Dialogue Approach is the most appropriate approach for a 21st Century psychology in crisis.},
	number = {4},
	journal = {Integr Psychol Behav Sci},
	author = {Lawrence, Jeanette A and Dodds, Agnes E},
	month = dec,
	year = {2010},
	note = {{PMID:} 20490955},
	keywords = {Behavioral Research, Ethics, Research, Human Experimentation, Humans, Measurement, Mental Disorders, Models, Psychological, Psychology, Psychometrics, Validity},
	pages = {299--309}
},

@article{lewis_psychophysical_1972,
	title = {Psychophysical and theoretical identifications},
	volume = {50},
	doi = {10.1080/00048407212341301},
	number = {3},
	journal = {Australasian Journal of Philosophy},
	author = {Lewis, David},
	year = {1972},
	keywords = {Construct, Neuroscience, Psychometrics},
	pages = {249--258}
},

@article{lissitz_further_2007,
	title = {Further Clarification regarding Validity and Education},
	volume = {36},
	url = {http://www.jstor.org/stable/4621104},
	doi = {10.2307/4621104},
	number = {8},
	urldate = {2013-01-27},
	journal = {Educational Researcher},
	author = {Lissitz, Robert W. and Samuelsen, Karen},
	month = nov,
	year = {2007},
	keywords = {Psychometrics, Validity},
	pages = {482--484}
},

@article{lovie_commentary_1997,
	title = {Commentary on Michell, Quantitative science and the definition of measurement in psychology},
	volume = {88},
	doi = {10.1111/j.2044-8295.1997.tb02644.x},
	number = {3},
	journal = {British Journal of Psychology},
	author = {Lovie, A. D.},
	year = {1997},
	keywords = {Measurement, Psychometrics},
	pages = {393–394}
},

@article{luce_quantification_1997,
	title = {Quantification and symmetry: Commentary on Michell, Quantitative science and the definition of measurement in psychology},
	volume = {88},
	shorttitle = {Quantification and symmetry},
	doi = {10.1111/j.2044-8295.1997.tb02645.x},
	abstract = {Several of Michell's points are amplified and emphasized and the following additional point is made. Most quantitative attributes can be measured in more than one way, and there are interesting questions about how they relate. Among other things, units of measurement and symmetries of the underlying structure may or may not {agree.Because} I agree with almost everything Michell says, my commentary is restricted to some amplification and to an added observation.},
	number = {3},
	journal = {British Journal of Psychology},
	author = {Luce, R. Duncan},
	year = {1997},
	keywords = {Measurement, Psychometrics},
	pages = {395–398}
},

@incollection{luce_representational_2002,
	title = {Representational Measurement Theory},
	url = {http://onlinelibrary.wiley.com.proxy.uchicago.edu/doi/10.1002/0471214426.pas0401/abstract},
	abstract = {This chapter focuses on empirical structures, with an ordering attribute, leading to numerical or geometric representations. A historical sketch of physical, behavioral, and social science measurement until 1950 is followed by modern approaches to representational measurement in the behavioral sciences. Among these structures are: difference, additive conjoint, averaging, and non-additive concatenation and conjoint structures. Derived measurement is described by distributive linkages between two structures with a common attribute and is exemplified by physical, utility, and psychophysical examples. Two mathematical results are needed in each case: the existence of structure-preserving numerical representations and their uniqueness. The latter has been analyzed generically. Using these ideas, models of magnitude estimation and production are {summarized.The} nature of axiomatization is discussed in terms of types of logical languages, the role of certain second-order axioms, and issues of consistency and independence of axioms. Important results about the impossibility of simple axiomatizations of classes of finite structures are described along with mention of some problems.},
	urldate = {2013-01-27},
	booktitle = {Stevens' Handbook of Experimental Psychology},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Luce, R. Duncan and Suppes, Patrick},
	year = {2002},
	keywords = {axiomatizability, derived measurement, error, invariance, meaningfulness, Measurement, Psychometrics, representational measurement, Representational theory of measurement, scale type}
},

@article{markus_constructs_2012,
	title = {Constructs and Attributes in Test Validity: Reflections on Newton's Account},
	volume = {10},
	shorttitle = {Constructs and Attributes in Test Validity},
	doi = {10.1080/15366367.2012.677348},
	number = {1-2},
	journal = {Measurement: Interdisciplinary Research \& Perspective},
	author = {Markus, Keith A.},
	year = {2012},
	keywords = {Construct, Latent variable, Validity},
	pages = {84--87}
},

@article{markus_science_1998,
	title = {Science, Measurement, and Validity: Is Completion of Samuel Messick's Synthesis Possible?},
	volume = {45},
	shorttitle = {Science, Measurement, and Validity},
	doi = {10.1023/A:1006960823277},
	abstract = {Messick's (1989) theory of test validity is profoundly influential (Hubley and Zumbo, 1996; Angoff, 1988) in part because it brings together disparate contributions into a unified framework for building validity arguments. At the heart of Messick's theory lies a synthesis of realism and constructivism with respect to both scientific facts and measurement. Within this synthesis there remains a tension between the evidential basis and the consequential basis for test interpretation and use. This cannot be sidestepped simply by limiting the evidential basis to test interpretation and the consequential basis to test use: Interpretation and use are not so easily held separate. The roles of constructivism and context in Messick's theory underline the inherent link between facts and values, but the assumption that facts are objective and values are subjective goes unquestioned in Messick's theory. The inherent link between facts and values combines with this assumption to produce the unresolved tension in Messick's theory. This suggests that a unified theory of test validity requires a theory of value justification.},
	number = {1/3},
	journal = {Social Indicators Research},
	author = {Markus, Keith A.},
	month = nov,
	year = {1998},
	keywords = {Measurement, Psychometrics, Validity},
	pages = {5--34}
},

@article{markus_cat_2012,
	title = {The cat came back: Evaluating arguments against psychological measurement},
	volume = {22},
	shorttitle = {The cat came back},
	doi = {10.1177/0959354310381155},
	abstract = {The possibility or impossibility of quantitative measurement in psychology has important ramifications for the nature of psychology as a discipline. Trendler’s (2009) argument for the impossibility of psychological measurement suggests a general and potentially fruitful strategy for further research on this question. However, the specific argument offered by Trendler appears flawed in several respects. It seems to conflate what must hold true with what one must know and also equivocate on the necessary evidence. Moreover, if the argument supported its conclusion, it would rule out qualitative discourse on psychology as well as psychological measurement. Taking Trendler’s argument as an example, one can formulate a general structure to arguments adopting the same basic strategy. An overview of the requirements that such arguments should meet provides a metatheoretical perspective that can assist authors in constructing such arguments and readers in critically evaluating them.},
	number = {4},
	journal = {Theory \& Psychology},
	author = {Markus, Keith A. and Borsboom, Denny},
	month = aug,
	year = {2012},
	keywords = {Measurement, Philosophy of Science, Psychometrics, Quantity, Validity},
	pages = {452--466}
},

@article{markus_reflective_2013,
	title = {Reflective measurement models, behavior domains, and common causes},
	volume = {31},
	shorttitle = {On defining and interpreting constructs: Ontological and epistemological constraints},
	doi = {10.1016/j.newideapsych.2011.02.008},
	abstract = {Causal theories of measurement view test items as effects of a common cause. Behavior domain theories view test item responses as behaviors sampled from a common domain. A domain score is a composite score over this domain. The question arises whether latent variables can simultaneously constitute domain scores and common causes of item scores. One argument to the contrary holds that behavior domain theory offers more effective guidance for item construction than a causal theory of measurement. A second argument appeals to the apparent circularity of taking a domain score, which is defined in terms of a domain of behaviors, as a cause of those behaviors. Both arguments require qualification and behavior domain theory seems to rely on implicit causal relationships in two respects. Three strategies permit reconciliation of the two theories: One can take a causal structure as providing the basis for a homogeneous domain. One can construct a homogeneous domain and then investigate whether a causal structure explains the homogeneity. Or, one can take the domain score as linked to an existing attribute constrained by indirect measurement.},
	number = {1},
	journal = {New Ideas in Psychology},
	author = {Markus, Keith A. and Borsboom, Denny},
	month = apr,
	year = {2013},
	keywords = {Behavior domain theory, Causal theory of measurement, Construct, Latent variable, Psychometrics},
	pages = {54--64}
},

@article{martin_positivism_2003,
	title = {Positivism, Quantification and the Phenomena of Psychology},
	volume = {13},
	doi = {10.1177/0959354303013001760},
	abstract = {While in general agreement with Michell's (2003) observations, arguments and positions, I believe two considerations might help to contextualize his piece further. First, it is important to note just how widespread have been psychologists' misunderstandings of positions and arguments in the philosophy of science, and what this says about the disciplinary isolation of psychology. Secondly, despite some common misunderstandings amongst qualitative researchers in psychology, there are good reasons for psychologists to resist both the quantitative imperative and the positivists' overly narrow construal of philosophy of science. These reasons relate to important, non-quantifiable characteristics of many psychological phenomena. Nonetheless, Michell's article is a timely reminder to guard against the excesses and limitations that attend any version of `methodolatry', quantitative or qualitative.},
	number = {1},
	journal = {Theory \& Psychology},
	author = {Martin, Jack},
	month = feb,
	year = {2003},
	keywords = {philosophy, positivism, Psychology, Psychometrics, Quantification, Science},
	pages = {33--38}
},

@article{messick_validity_1995,
	title = {Validity of psychological assessment: Validation of inferences from persons' responses and performances as scientific inquiry into score meaning},
	volume = {50},
	shorttitle = {Validity of psychological assessment},
	doi = {10.1037/0003-066X.50.9.741},
	abstract = {The traditional conception of validity divides it into three separate and substitutable types: content, criterion, and construct validities. This view is fragmented and incomplete, especially because it fails to take into account both evidence of the value implications of score meaning as a basis for action and the social consequences of score use. The new unified concept of validity interrelates these issues as fundamental aspects of a more comprehensive theory of construct validity that addresses both score meaning and social values in test interpretation and test use. That is, unified validity integrates considerations of content, criteria, and consequences into a construct framework for the empirical testing of rational hypotheses about score meaning and theoretically relevant relationships, including those of an applied and a scientific nature. Six distinguishable aspects of construct validity are highlighted as a means of addressing central issues implicit in the notion of validity as a unified concept. These are content, substantive, structural, generalizability, external , and consequential aspects of construct validity. In effect, these six aspects function as general validity criteria or standards for all educational and psychological measurement, including performance assessments, which are discussed in some detail because of their increasing emphasis in educational and employment settings. ({PsycINFO} Database Record (c) 2010 {APA}, all rights reserved)},
	number = {9},
	journal = {American Psychologist},
	author = {Messick, Samuel},
	year = {1995},
	keywords = {Educational Measurement, Psychometrics, Validity},
	pages = {741--749}
},

@article{michell_constructs_2013,
	title = {Constructs, inferences, and mental measurement},
	volume = {31},
	shorttitle = {On defining and interpreting constructs: Ontological and epistemological constraints},
	doi = {10.1016/j.newideapsych.2011.02.004},
	abstract = {The ‘construct’ concept occupies a significant place in psychology and, yet its role is misunderstood. Psychologists think that theorising in the area of psychological testing involves conjuring constructs, which are operationally defined and measured via psychometric tests. However, the ‘construct’ concept is unworkable and laden with confused philosophical baggage accrued under the hegemony of logical empiricism, and its real function in psychology is obscured. Via an analysis of its history and logic, I expose its flawed conception of the relation between theoretical and observable concepts and the way in which it serves the myth of mental measurement. Finally, it is shown how the actual logic of theorising in science, which entails that theories are best inferred from relevant phenomena, not imaginatively constructed, oppugns this myth and promises to coordinate theoretical concepts with the phenomena to be explained.},
	number = {1},
	journal = {New Ideas in Psychology},
	author = {Michell, Joel},
	month = apr,
	year = {2013},
	keywords = {Construct, Continuous quantity, Latent variable, Measurement, Order, Psychometrics, Theoretical concept},
	pages = {13--21}
},

@article{michell_item_2004,
	title = {Item Response Models, Pathological Science and the Shape of Error Reply to Borsboom and Mellenbergh},
	volume = {14},
	doi = {10.1177/0959354304040201},
	abstract = {There is nothing in Borsboom and Mellenbergh’s (2004) response that refutes my thesis that psychometrics is a pathology of science. They seek to defend item response models from my charge of pathological science without apparently realizing that my charge relates to psychometricians, not to models. They appeal to the Quine-Duhem thesis in an attempt to argue that item response models do not allow the hypothesis that psychological attributes are quantitative to be tested in isolation, but their argument is based upon a misinterpretation of Duhem. In any experiment, what is being tested depends on what the experimenter already takes to be true, and it is possible that a psychometrician could be testing just one of the hypotheses constituting an item response model. Furthermore, using the theory of conjoint measurement, it is possible to isolate predictions that depend upon psychological attributes being quantitative, as opposed to merely ordinal. Despite this, Borsboom and Mellenbergh agree with the first part of my thesis. They do not discuss the second part, but an examination of textbooks on item response models shows that psychometricians disguise their failure to test the hypothesis that psychological attributes are quantitative by simply declining to mention that this hypothesis is presumed in their models. Claims to measure psychological attributes based upon these models depend exclusively upon the weakest part of these models: the hypothesis that the distribution of ‘errors’ takes a specific form.},
	number = {1},
	journal = {Theory \& Psychology},
	author = {Michell, Joel},
	month = feb,
	year = {2004},
	keywords = {{IRT}, item response models, pathological science, Psychological measurement, Psychometrics, Quantification},
	pages = {121--129}
},

@article{michell_constantly_2012,
	title = {{``The} constantly recurring argument'': Inferring quantity from order},
	volume = {22},
	shorttitle = {{“The} constantly recurring argument”},
	doi = {10.1177/0959354311434656},
	abstract = {The inference from order to quantity is fundamental to psychometrics because the sorts of attributes that psychometricians aspire to measure are experienced directly only as ordered and, yet, it is concluded that such attributes are measurable on interval scales (i.e., that they are quantitative). This inference has been a feature of psychometrics since early last century, before which it permeated scientific thought and played a role in the development of psychophysics. Despite this, its cogency has been analysed only rarely. Elsewhere, I have argued that it is not deductively valid, a point that might be considered obvious except that attempts have been made to show otherwise. Its invalidity displayed, it is easily shown that it is not inductively reasonable either. However, it might still be urged that the inference from order to quantity is an inference to the best explanation: that is, that quantitative structure is reasonably abduced from order. I argue that the opposite is true: the most plausible hypothesis is that the sorts of attributes psychometricians aspire to measure are merely ordinal attributes with impure differences of degree, a feature logically incompatible with quantitative structure. If so, psychometrics is built upon a myth.},
	number = {3},
	journal = {Theory \& Psychology},
	author = {Michell, Joel},
	month = jun,
	year = {2012},
	keywords = {Measurement, Order, Psychometrics, Quality, Quantity},
	pages = {255--271}
},

@article{michell_psychometricians_2009,
	title = {The psychometricians' fallacy: Too clever by half?},
	volume = {62},
	shorttitle = {The psychometricians' fallacy},
	doi = {10.1348/000711007X243582},
	abstract = {The psychometricians' fallacy concludes that an attribute is quantitative from the premise that it is ordinal. This fallacy occupies a central place in the paradigm of psychometrics. Most of the founders of the discipline committed it and it makes sense of otherwise anomalous developments within the discipline, such as the permissible statistics controversy and the dominant form taken by item response theories. The fallacy is displayed by showing (1) that an attribute's quantitative structure reduces to a weak order upon differences between degrees that satisfies the double cancellation, solvability, and Archimedean conditions of conjoint measurement theory and (2) the fact that any order on the degrees themselves does not entail sufficient structure on this weak order to guarantee satisfaction of these conditions. Thus, it is possible that an ordered attribute is non-quantitative. Also, each pair of differences between degrees of an ordinal attribute falls into one of two disjoint classes: (1) those where the order relation between the pair follows from an order on the attribute and (2) those where it is independent of that order and possibly diagnostic of quantitative structure and this fact means that the distinction between order and quantity is an empirical one.},
	number = {1},
	journal = {British Journal of Mathematical and Statistical Psychology},
	author = {Michell, Joel},
	year = {2009},
	keywords = {Psychometrics},
	pages = {41–55}
},

@article{morgan_measurement_1997,
	title = {Measurement in psychology: Commentary on Michell's Quantitative Science and the definition of measurement in psychology},
	volume = {88},
	shorttitle = {Measurement in psychology},
	doi = {10.1111/j.2044-8295.1997.tb02646.x},
	number = {3},
	journal = {British Journal of Psychology},
	author = {Morgan, Michael},
	year = {1997},
	keywords = {Measurement, Psychometrics},
	pages = {399–400}
},

@article{poole_routine_1994,
	title = {Routine Testing Practices and the Linguistic Construction of Knowledge},
	volume = {12},
	doi = {10.1207/s1532690xci1202_3},
	abstract = {This article examines the linguistic encoding of curricular knowledge in routine classroom testing events. Focusing on transcript data collected in a qualitative study of junior high school social studies classrooms, I argue that the dominant epistemological orientation of testing events is positivistic and values a discrete, bounded form of knowledge. The analysis centers on the language of review activities that typically precede and follow classroom tests; specifically, it focuses on interactional sequences that demand students' verbal participation in a culturally specified orientation to knowledge. A comparison of the language of these testing events and earlier lesson presentations of the same curricular information suggests that testing encourages and exaggerates the extent to which a positivistic view of knowledge prevails.},
	number = {2},
	journal = {Cognition and Instruction},
	author = {Poole, Deborah},
	month = jan,
	year = {1994},
	keywords = {Ideology, Interactional linguistics, Pragmatics, Psychometrics, Sociology of Knowledge, Testing},
	pages = {125--150}
},

@article{raatikainen_causation_2010,
	title = {Causation, Exclusion, and the Special Sciences},
	volume = {73},
	doi = {10.1007/s10670-010-9236-0},
	number = {3},
	journal = {Erkenn},
	author = {Raatikainen, Panu},
	month = nov,
	year = {2010},
	keywords = {Causality, Epistemology, Ethics, Latent variable, Logic, ontology, philosophy, Psychometrics},
	pages = {349--363}
},

@article{rosenbaum_-making_2011,
	title = {The un-making of a method: From rating scales to the study of psychological processes},
	volume = {21},
	shorttitle = {The un-making of a method},
	doi = {10.1177/0959354309352913},
	abstract = {Rating scales are standard instruments in psychology. They force the research participant to provide a numerical estimate of an assumed “degree” of some characteristic along a linear scale. We prove that such numerical estimates are artifacts based on unknown psychological processes that are used in the making of a rating. Psychology’s current use of rating scales entails reliance upon unexplored and abbreviated introspection. It superimposes upon the rater the use of real numbers for the subjective construction of the ratings. The axiomatic superimposition of the notion of “degree” of subjective estimates by the rating task overlooks the qualitative (structural) relation between the implied opposites. We propose the reconstruction of the rating tasks into a method that accesses the process of meaning construction by the rater. When the rater faces a rating task, a field of meanings is constructed in terms of dialogical oppositions. These oppositions can be observed to lead to the moment of subjective synthesis (the rating outcome). Examples are given of the tracing of the process of subjective synthesis from an empirical study using {NEO} {PI} items. We claim that reconstruction of the rating task in terms of the study of microgenesis of rating processes allows psychology access to the reality of the workings of the human mind.},
	number = {1},
	journal = {Theory \& Psychology},
	author = {Rosenbaum, Philip J. and Valsiner, Jaan},
	month = feb,
	year = {2011},
	keywords = {dialogical processes, Introspection, Measurement, Psychometrics, Quality, rating scales},
	pages = {47--65}
},

@article{ruck_stale_2010,
	title = {A Stale Challenge to the Philosophy of Science},
	volume = {44},
	doi = {10.1007/s12124-010-9121-z},
	abstract = {In his article {“Is} psychology based on a methodological error?” and based on a quite convincing empirical basis, Michael Schwarz offers a methodological critique of one of mainstream psychology’s key test theoretical axioms, i.e., that of the in principle normal distribution of personality variables. It is characteristic of this paper—and at first seems to be a strength of it—that the author positions his critique within a frame of philosophy of science, particularly positioning himself in the tradition of Karl Popper’s critical rationalism. When scrutinizing Schwarz’s arguments, however, we find Schwarz’s critique profound only as an immanent critique of test theoretical axioms. We raise doubts, however, as to Schwarz’s alleged ‘challenge’ to the philosophy of science because the author not at all seems to be in touch with the state of the art of contemporary philosophy of science. Above all, we question the universalist undercurrent that Schwarz’s ‘bio-psycho-social model’ of human judgment boils down to. In contrast to such position, we close our commentary with a plea for a context- and culture sensitive philosophy of science.},
	number = {2},
	journal = {Integr. psych. behav.},
	author = {Ruck, Nora and Slunecko, Thomas},
	month = jun,
	year = {2010},
	keywords = {Anthropology, Bio-psycho-social model, Critique of psychology, cultural psychology, Measurement, {METHODOLOGY}, Philosophy of Science, Psychology, general, Psychometrics, Sociology},
	pages = {168--175}
},

@article{schmittmann_deconstructing_2013,
	title = {Deconstructing the construct: A network perspective on psychological phenomena},
	volume = {31},
	shorttitle = {On defining and interpreting constructs: Ontological and epistemological constraints},
	doi = {10.1016/j.newideapsych.2011.02.007},
	abstract = {In psychological measurement, two interpretations of measurement systems have been developed: the reflective interpretation, in which the measured attribute is conceptualized as the common cause of the observables, and the formative interpretation, in which the measured attribute is seen as the common effect of the observables. We advocate a third interpretation, in which attributes are conceptualized as systems of causally coupled (observable) variables. In such a view, a construct like ’depression’ is not seen as a latent variable that underlies symptoms like ’lack of sleep’ or ’fatigue’, and neither as a composite constructed out of these symptoms, but as a system of causal relations between the symptoms themselves (e.g., lack of sleep → fatigue, etc.). We discuss methodological strategies to investigate such systems as well as theoretical consequences that bear on the question in which sense such a construct could be interpreted as real.},
	number = {1},
	journal = {New Ideas in Psychology},
	author = {Schmittmann, Verena D. and Cramer, Angélique {O.J.} and Waldorp, Lourens J. and Epskamp, Sacha and Kievit, Rogier A. and Borsboom, Denny},
	month = apr,
	year = {2013},
	keywords = {Construct, Dynamical systems, Formative model, Latent variable, network, Psychological construct, Psychometrics, Reflective model},
	pages = {43--53}
},

@article{sijtsma_future_2012,
	title = {Future of Psychometrics: Ask What Psychometrics Can Do for Psychology},
	volume = {77},
	shorttitle = {Future of Psychometrics},
	doi = {10.1007/s11336-011-9242-4},
	abstract = {I address two issues that were inspired by my work on the Dutch Committee on Tests and Testing ({COTAN).} The first issue is the understanding of problems test constructors and researchers using tests have of psychometric knowledge. I argue that this understanding is important for a field, like psychometrics, for which the dissemination of psychometric knowledge among test constructors and researchers in general is highly important. The second issue concerns the identification of psychometric research topics that are relevant for test constructors and test users but in my view do not receive enough attention in psychometrics. I discuss the influence of test length on decision quality in personnel selection and quality of difference scores in therapy assessment, and theory development in test construction and validity research. I also briefly mention the issue of whether particular attributes are continuous or discrete.},
	number = {1},
	journal = {Psychometrika},
	author = {Sijtsma, Klaas},
	month = jan,
	year = {2012},
	keywords = {Assessment, Testing and Evaluation, change assessment, decision quality based on short tests, didactics of psychometrics, personnel selection, Psychometrics, Statistical Theory and Methods, Statistics for Social Science, Behavorial Science, Education, Public Policy, and Law, test validity, test-quality assessment, theory construction},
	pages = {4--20}
},

@article{sijtsma_psychological_2012,
	title = {Psychological measurement between physics and statistics},
	volume = {22},
	doi = {10.1177/0959354312454353},
	abstract = {This contribution discusses the physical perspective on psychological measurement represented by additive conjoint measurement and the statistical perspective represented by item response theory, and argues that both fail to adequately address the real measurement problem in psychology: this is the absence of well-developed theories about psychological attributes. I argue that the two perspectives leave psychology out of the equation and by doing so come up with proposals for psychological measurement that are fruitless. Only the rigorous development of attribute theories can lead to meaningful measurement. I provide two examples of the measurement of theoretically well-developed attributes and suggest future directions for psychological measurement.},
	number = {6},
	journal = {Theory \& Psychology},
	author = {Sijtsma, Klaas},
	month = dec,
	year = {2012},
	keywords = {additive conjoint measurement, item response theory, Measurement, physical perspective on measurement, practical test construction, Psychometrics, Rasch model, statistical perspective on measurement},
	pages = {786--809}
},

@incollection{steyer_classical_2001,
	address = {Oxford},
	title = {Classical (Psychometric) Test Theory},
	doi = {10.1016/B0-08-043076-7/00721-X},
	abstract = {Classical Test Theory (CTT) has been developed to quantify measurement error and to solve related problems such as correcting observed dependencies between variables (e.g., correlations) for the attenuation due to measurement errors. Basic concepts of CTT are true score and measurement error variables. These concepts are defined as specific conditional expectations and its residual, respectively. The definitions of these concepts already imply a number of properties that were considered axioms in early presentations of CTT. Models of CTT consist of assumptions about the true score and error variables allowing to identify the theoretical parameters (such as true score variance and error variance) from the variances and covariance's of the observable measurements (test score variables). A number of implications of the assumptions defining models of CTT may be tested empirically via structural equation modeling. Hinting at more recent theories and their goals such as Item Response Theory, Generalizability Theory, and Latent State-Trait Theory concludes this article.},
	urldate = {2013-02-18},
	booktitle = {International Encyclopedia of the Social \& Behavioral Sciences},
	publisher = {Pergamon},
	author = {Steyer, R.},
	editor = {Editors-in-Chief:  Neil J. Smelser and Paul B. Baltes},
	year = {2001},
	keywords = {{CTT}, Psychometrics, Test theory},
	pages = {1955--1962}
},

@article{thurstone_law_1927,
	title = {A Law of Comparative Judgment},
	volume = {34},
	doi = {10.1037/h0070288},
	abstract = {A new psychological law, called the law of comparative judgment, is presented with some of its special applications in the measurement of psychological values. This law is applicable not only to the comparison of physical stimulus intensities but also to qualitative comparative judgments, such as those of excellence of specimens in an educational scale. It should be possible also to verify it on comparative judgments which involve simultaneous and successive contrast. The law is stated as follows:[Equation omitted]in which S1 and S2 are the psychological scale values of the two compared stimuli; x12 is the sigma value corresponding to the proportion of judgments p1 {\textgreater} p2. ς1 is the discriminal dispersion of stimulus R1 and ς2 is the dispersion of stimulus R2. r is the correlation between the discriminal deviations of R1 and R2 in the same judgment. This law is basic for work on Weber's and Fechner's laws, applies to the judgments of a single observer who compares a series of stimuli by the method of paired comparisons when no "equal" judgments are allowed, and is a rational equation for the method of constant stimuli. The law is then applied to five cases each of which involves different assumptions and different degrees of simplification of the law for practical use. The weighting of the observation equations is discussed because the observation equations obtained with the five cases are not of the same reliability and hence should not be equally weighted.},
	number = {4},
	journal = {Psychological Review},
	author = {Thurstone, L. L.},
	year = {1927},
	keywords = {comparative judgment, psychological values, Psychometrics},
	pages = {273--286}
},

@article{van_orden_situated_2010,
	title = {Situated Behavior and the Place of Measurement in Psychological Theory},
	volume = {22},
	doi = {10.1080/10407410903493145},
	abstract = {Measured values of human behavior may entail contradictory attributes of wave and particle by analogy with the wave/particle attributes of the electron. 1/f scaling is the wave attribute in this analogy and punctate data points are the particle attribute. One consequence of the wave/particle duality in physics was to elevate measurement to a primary place in physical theory, and one purpose of the present analogy is to likewise elevate measurement to a primary place in psychological theory. Another purpose is to emulate Robert Shaw's creative use of analogies, consistent with the brief quotation that begins this article.},
	number = {1},
	journal = {Ecological Psychology},
	author = {Van Orden, Guy C. and Kello, Christopher T. and Holden, John G.},
	year = {2010},
	keywords = {Distributed Cognition, Dynamic Systems, Measurement, Psychometrics, Situation},
	pages = {24--43}
},

@article{vautier_ambiguous_2012,
	title = {The ambiguous utility of psychometrics for the interpretative foundation of socially relevant avatars},
	volume = {22},
	doi = {10.1177/0959354312450093},
	abstract = {The persisting debates that measurement in psychology elicits can be explained by the conflict between two aspiration types. One, the epistemologic aspiration, resting on the search for scientific truth, and two, the social aspiration, resting on the demonstration of a capacity to contribute to psychological assessment problems in particular. Psychometrics answer essentially to psychology’s demand for social utility, leading to the quasi-exclusive attribution of importance to quantitative interpretation. For psychology to be considered an empirical science, it has to establish its capacity for the measurement of psychological phenomena, even if this means that it recognizes that these phenomena are essentially qualitative.},
	number = {6},
	journal = {Theory \& Psychology},
	author = {Vautier, Stéphane and Veldhuis, Michiel and Lacot, Émilie and Matton, Nadine},
	month = dec,
	year = {2012},
	keywords = {assessment, Classical test theory, item response modeling, Measurement, Psychometrics},
	pages = {810--822}
},

@article{watzlawik_weisheits_2009,
	title = {{“Der} Weisheits letzter Schluss”? Wisdom’s Last Conclusion?},
	volume = {43},
	shorttitle = {{“Der} Weisheits letzter Schluss”?},
	doi = {10.1007/s12124-009-9094-y},
	abstract = {Unexpected empirical findings lead Schwarz (2009) to question current methodological approaches within psychology. He claims that distribution anomalies, which cannot be explained by independent variables, actually prove an error in classical test theory, which then leads him to criticize current scientific conventionalism. In this commentary, it is shown that the current university system not only uses, but often reproduces conventionalism. It is, of course, necessary to teach certain techniques and tools to future psychologists, but using these critically seem to be essential in preventing scientific approaches turn into ideologies that are or must not be questioned—even if this is sometimes less threatening to one’s identity as a scientist/psychologist. This is true for all sciences, but understanding the bio-psycho-social interdependencies of human nature (ambiguities), as Schwarz describes it, seems to be a particular challenge that is in many ways different from other disciplines. Instead of striving to be like them, it is suggested we establish an ambiguity-accepting and critical environment in psychology in which theories and approaches are not considered final but temporary to foster change and progress.},
	number = {3},
	journal = {Integr. psych. behav.},
	author = {Watzlawik, Meike},
	month = sep,
	year = {2009},
	keywords = {Ambiguity, Anthropology, Classical test theory, Conventionalism, Crisis, Indoctrination, Methodological approaches, Philosophy in psychology, Psychology, general, Psychometrics, Sociology},
	pages = {214--220}
},

@article{westerman_conversation_2011,
	title = {Conversation analysis and interpretive quantitative research on psychotherapy process and problematic interpersonal behavior},
	volume = {21},
	doi = {10.1177/0959354310394719},
	abstract = {In this article, I examine conversation analysis, a fruitful area of qualitative research, in order to extend my prior explorations of the idea that quantitative methods can and should be part of the repertoire of interpretive approaches employed by investigators committed to treating psychological phenomena as irreducibly meaningful. My examination includes considering several lines of research by investigators who are not practitioners of conversation analysis in which quantitative methods were employed to study patient behavior in psychotherapy and defensive behavior more generally. These lines of inquiry show that (a) quantitative research methods have a good deal to offer practitioners of conversation analysis as they endeavor to advance our understanding of the organization of interactions, and (b) we can employ quantitative methods and continue to embrace a commitment to interpretive inquiry. I also offer a critique of fundamental methodological precepts associated with conversation analysis, which differ notably from the precepts guiding most qualitative research efforts in psychology. In a fascinating twist, these precepts, which include discomfort with interpretive research procedures, have resulted in limitations in very recent attempts by some practitioners of conversation analysis to employ quantitative methods in their investigations.},
	number = {2},
	journal = {Theory \& Psychology},
	author = {Westerman, Michael A.},
	month = apr,
	year = {2011},
	keywords = {Conversation analysis, dyselaboration, interpersonal defense, Interpretation, patient coordination, Psychometrics, qualitative research, quantitative methods},
	pages = {155--178}
},

@article{westerman_changing_2011,
	title = {Changing the terms of the debate: Quantitative methods in explicitly interpretive research},
	volume = {21},
	shorttitle = {Changing the terms of the debate},
	doi = {10.1177/0959354310393565},
	abstract = {We introduce this special issue by arguing that quantitative and qualitative research methods do not line up neatly with the guiding philosophical commitments of the two sides of the schism in the field between the mainstream, natural science approach, and the minority, human science position. This leads to the motivating idea for the issue, the view that quantitative methods, when used appropriately, can contribute to interpretive inquiry in psychology. We discuss the issue’s two main objectives—(1) presenting lines of actual research that illustrate how quantitative approaches can be used in ways that are consistent with a human science approach to the field, and (2) providing critical examination of the motivating idea—and introduce the articles in the issue that address each of these objectives. In the final section of this introductory article, we offer our thoughts about what the full set of papers accomplishes and suggest that the issue’s many-sided exploration goes some distance toward changing the terms of the quantitative—qualitative debate.},
	number = {2},
	journal = {Theory \& Psychology},
	author = {Westerman, Michael A. and Yanchar, Stephen C.},
	month = apr,
	year = {2011},
	keywords = {explicitly interpretive quantitative research, Interpretation, Psychometrics, qualitative research, quantitative methods, science wars},
	pages = {139--154}
},

@article{zumbo_opening_1998,
	title = {Opening Remarks to the Special Issue on Validity Theory and the Methods Used in Validation: Perspectives from the Social and Behavioral Sciences},
	volume = {45},
	shorttitle = {Opening Remarks to the Special Issue on Validity Theory and the Methods Used in Validation},
	doi = {10.1023/A:1006956722368},
	number = {1/3},
	journal = {Social Indicators Research},
	author = {Zumbo, Bruno D.},
	month = nov,
	year = {1998},
	keywords = {Measurement, Psychometrics, Special Issue, Validity},
	pages = {1--3}
}